{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbe4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d46b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#===================================  SUBFUNCTIONS  ======================================================\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "def peakiness(waves, **kwargs):  \n",
    "    \n",
    "    \"finds peakiness of waveforms.\" \n",
    "\n",
    "    #print(\"Beginning peakiness\")\n",
    "    # Kwargs are:\n",
    "    #          wf_plots. specify a number n: wf_plots=n, to show the first n waveform plots. \\\n",
    "     \n",
    "    import numpy as np\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import time\n",
    "\n",
    "    print(\"Running peakiness function...\")\n",
    "    size=np.shape(waves)[0]\n",
    "    \n",
    "    waves1=np.copy(waves)\n",
    "     \n",
    "    if waves1.ndim == 1:\n",
    "        print('only one waveform in file')\n",
    "        waves2=waves1.reshape(1,np.size(waves1))\n",
    "        waves1=waves2\n",
    "\n",
    "    def by_row(waves, *args):\n",
    "        \"calculate peakiness for each waveform\"\n",
    "        maximum=np.nanmax(waves)\n",
    "        if maximum > 0:\n",
    "\n",
    "            maximum_bin=np.where(waves==maximum)\n",
    "            #print(maximum_bin)\n",
    "            maximum_bin=maximum_bin[0][0]\n",
    "            waves_128=waves[maximum_bin-50:maximum_bin+78]\n",
    "\n",
    "            waves=waves_128\n",
    "\n",
    "            noise_floor=np.nanmean(waves[10:20])\n",
    "            where_above_nf=np.where(waves > noise_floor)\n",
    "\n",
    "            if np.shape(where_above_nf)[1] > 0:\n",
    "                maximum=np.nanmax(waves[where_above_nf])\n",
    "                total=np.sum(waves[where_above_nf])\n",
    "                mean=np.nanmean(waves[where_above_nf])\n",
    "                peaky=maximum/mean\n",
    "\n",
    "            else:\n",
    "                peaky = np.nan\n",
    "                maximum = np.nan\n",
    "                total = np.nan\n",
    "\n",
    "        else:\n",
    "            peaky = np.nan\n",
    "            maximum = np.nan\n",
    "            total = np.nan\n",
    "\n",
    "        if 'maxs' in args:\n",
    "            return maximum\n",
    "        if 'totals' in args:\n",
    "            return total\n",
    "        if 'peaky' in args:\n",
    "            return peaky\n",
    "\n",
    "    peaky=np.apply_along_axis(by_row, 1, waves1, 'peaky')\n",
    "\n",
    "    if 'wf_plots' in kwargs:\n",
    "        maximums=np.apply_along_axis(by_row, 1, waves1, 'maxs')\n",
    "        totals=np.apply_along_axis(by_row, 1, waves1, 'totals')\n",
    "\n",
    "        for i in range(0,kwargs['wf_plots']):\n",
    "            if i == 0:\n",
    "                print(\"Plotting first \"+str(kwargs['wf_plots'])+\" waveforms\")\n",
    "\n",
    "            plt.plot(waves1[i,:])#, a, col[i],label=label[i])\n",
    "            plt.axhline(maximums[i], color='green')\n",
    "            plt.axvline(10, color='r')\n",
    "            plt.axvline(19, color='r')\n",
    "            plt.xlabel('Bin (of 256)')\n",
    "            plt.ylabel('Power')\n",
    "            plt.text(5,maximums[i],\"maximum=\"+str(maximums[i]))\n",
    "            plt.text(5,maximums[i]-2500,\"total=\"+str(totals[i]))\n",
    "            plt.text(5,maximums[i]-5000,\"peakiness=\"+str(peaky[i]))\n",
    "            plt.title('waveform '+str(i)+' of '+str(size)+'\\n. Noise floor average taken between red lines.')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    return peaky\n",
    "\n",
    "#=========================================================================================================\n",
    "#=========================================================================================================\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "def unpack_gpod(variable):\n",
    "        \n",
    "    from scipy.interpolate import interp1d\n",
    "        \n",
    "    time_1hz=SAR_data.variables['time_counter_1Hz'][:]\n",
    "    time_20hz=SAR_data.variables['time_counter_20Hz'][:]\n",
    "\n",
    "    out=(SAR_data.variables[variable][:]).astype(float)  # convert from integer array to float.\n",
    "\n",
    "    #if ma.is_masked(dataset.variables[variable][:]) == True:\n",
    "    #print(variable,'is masked. Removing mask and replacing masked values with nan')\n",
    "    out=np.ma.filled(out, np.nan) \n",
    "        \n",
    "    if len(out)==len(time_1hz):\n",
    "            \n",
    "        print(variable,'is 1hz. Expanding to 20hz...')\n",
    "        out = interp1d(time_1hz,out,fill_value=\"extrapolate\")(time_20hz)  \n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "#=========================================================================================================\n",
    "#=========================================================================================================\n",
    "#=========================================================================================================\n",
    "\n",
    "def calculate_SSD(RIP):\n",
    "\n",
    "    from scipy.optimize import curve_fit\n",
    "    from scipy import asarray as ar,exp\n",
    "    do_plot='Off'\n",
    "    \n",
    "    def gaussian(x,a,x0,sigma):\n",
    "            return a * np.exp(-(x - x0)**2 / (2 * sigma**2))\n",
    "    \n",
    "    SSD=np.zeros(np.shape(RIP)[0])*np.nan\n",
    "    x=np.arange(np.shape(RIP)[1])\n",
    "    \n",
    "    for i in range(np.shape(RIP)[0]):\n",
    "    \n",
    "        y=np.copy(RIP[i])\n",
    "        y[(np.isnan(y)==True)]=0\n",
    "\n",
    "        if 'popt' in locals():\n",
    "            del(popt,pcov)\n",
    "\n",
    "        SSD_calc=0.5*(np.sum(y**2)*np.sum(y**2)/np.sum(y**4))\n",
    "        #print('SSD calculated from equation',SSD)\n",
    "\n",
    "        #n = len(x)                          \n",
    "        mean_est = sum(x * y) / sum(y)\n",
    "        sigma_est = np.sqrt(sum(y * (x - mean_est)**2) / sum(y))\n",
    "        #print('est. mean',mean,'est. sigma',sigma_est)\n",
    "\n",
    "        try:\n",
    "            popt,pcov = curve_fit(gaussian, x, y, p0=[max(y), mean_est, sigma_est],maxfev=10000)\n",
    "        except RuntimeError as e:\n",
    "            print(\"Gaussian SSD curve-fit error: \"+str(e))\n",
    "            #plt.plot(y)\n",
    "            #plt.show()\n",
    "\n",
    "        except TypeError as t:\n",
    "            print(\"Gaussian SSD curve-fit error: \"+str(t))\n",
    "\n",
    "        if do_plot=='ON':\n",
    "\n",
    "            plt.plot(x,y)\n",
    "            plt.plot(x,gaussian(x,*popt),'ro:',label='fit')\n",
    "            plt.axvline(popt[1])\n",
    "            plt.axvspan(popt[1]-popt[2], popt[1]+popt[2], alpha=0.15, color='Navy')\n",
    "            plt.show()\n",
    "\n",
    "            print('popt',popt)\n",
    "            print('curve fit SSD',popt[2])\n",
    "\n",
    "        if 'popt' in locals():\n",
    "            SSD[i]=abs(popt[2])\n",
    "            \n",
    "    \n",
    "    return SSD\n",
    "#=========================================================================================================\n",
    "#=========================================================================================================\n",
    "#=========================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy.ma as ma\n",
    "import glob\n",
    "from matplotlib.patches import Polygon\n",
    "import scipy.spatial as spatial\n",
    "from scipy.spatial import KDTree\n",
    "import netCDF4\n",
    "\n",
    "# dir_path='/Users/dorsa/Downloads/ProcessIRL_S3_SAR_over_OLCI/'\n",
    "dir_path=''\n",
    "\n",
    "with open(dir_path+'select_files_test.txt') as f:\n",
    "    selects = [line.rstrip('\\n') for line in open(dir_path+'select_files_test.txt')]\n",
    " \n",
    "with open(dir_path+'matching_SAR_tracks_test.txt') as f:\n",
    "    matching_SAR_list = [line.rstrip('\\n') for line in open(dir_path+'matching_SAR_tracks_test.txt')]\n",
    "\n",
    "j=-1\n",
    "\n",
    "CNN_training={}\n",
    "\n",
    "# nested \n",
    "# https://stackoverflow.com/questions/10756427/loop-through-all-nested-dictionary-values\n",
    "# dic = {}\n",
    "# dic[\"key1\"] = {}\n",
    "# dic[\"key1\"][\"key1.1\"] = \"value1\"\n",
    "# selects=glob.glob('/Volumes/cpnet/li1_cpdata/SATS/OPTICAL/OLCI/selects/*.png')\n",
    "\n",
    "OLCI_file_list_orig=[]\n",
    "OLCI_file_list=[]\n",
    "for OLCI_file_orig in selects[0:18]:\n",
    "    OLCI_file_list_orig.append(OLCI_file_orig)\n",
    "    startindex=(OLCI_file_orig.index('S3')) # start of the filenmaes in selects list indexed as 1  \n",
    "    OLCI_file_list.append(OLCI_file_orig[startindex:-9]) # [startindex:-9] takes the .lres.png off the end of the filenames  \n",
    "   \n",
    "SAR_file_list=[]\n",
    "SL_flagfile_list=[]\n",
    "for OLCI_file in OLCI_file_list: \n",
    "    CNN_training[OLCI_file]={} \n",
    "       \n",
    "    j=j+1\n",
    "#    startindex=(file.index('S3A_OL_1_EFR_'))\n",
    "#    OLCI_file='/Volumes/cpnet/li1_cpdata/SATS/OPTICAL/OLCI/'+file[startindex:-9]\n",
    "#    OLCI_file='/Volumes/OLCI/'+file[startindex:-9]\n",
    "    OLCI_file_p=dir_path+'OLCI_test/'+OLCI_file  # pathhway to OLCI_file\n",
    "    print('OLCI file is:', selects[j])\n",
    "    tif_file=OLCI_file_p+'/true_color.tif'\n",
    "    # Load in instrument data \n",
    "    instrument_data = netCDF4.Dataset(OLCI_file_p+'/instrument_data.nc')\n",
    "    solar_flux = instrument_data.variables['solar_flux'][:]\n",
    "    solar_flux_Band_Oa01 = solar_flux[0] # Band 1 has index 0 ect. \n",
    "    detector_index = instrument_data.variables['detector_index'][:]\n",
    "\n",
    "# Load in tie geometries\n",
    "    tie_geometries = netCDF4.Dataset(OLCI_file_p+'/tie_geometries.nc')\n",
    "    SZA = tie_geometries.variables['SZA'][:]\n",
    "    \n",
    "    geo_coords=Dataset(OLCI_file_p+'/geo_coordinates.nc')\n",
    "    OLCI_lat=geo_coords['latitude'][:]\n",
    "    OLCI_lon=geo_coords['longitude'][:]\n",
    "    OLCI_lon_r=OLCI_lon.ravel()\n",
    "    OLCI_lat_r=OLCI_lat.ravel()\n",
    " \n",
    "    # loop over radiance bands for OLCI_data array:        \n",
    "    OLCI_data=[]\n",
    "    for Radiance in range(1,22):\n",
    "        Rstr=\"%02d\" % Radiance\n",
    "        OLCI_nc=Dataset(OLCI_file_p+'/Oa'+Rstr+'_radiance.nc')\n",
    "        width = instrument_data.dimensions['columns'].size\n",
    "        height = instrument_data.dimensions['rows'].size\n",
    "        TOA_BRF = np.zeros((height, width), dtype='float32')\n",
    "        angle=np.zeros((TOA_BRF.shape[0],TOA_BRF.shape[1]))\n",
    "        for x in range(TOA_BRF.shape[1]):\n",
    "          angle[:,x]=SZA[:,int(x/64)]\n",
    "\n",
    "        width = instrument_data.dimensions['columns'].size\n",
    "        height = instrument_data.dimensions['rows'].size\n",
    "        TOA_BRF = np.zeros((height, width), dtype=float)\n",
    "        TOA_BRF=np.pi*np.asarray(OLCI_nc['Oa'+Rstr+'_radiance'])/solar_flux_Band_Oa01[detector_index]/np.cos(np.radians(angle))\n",
    "        \n",
    "        OLCI_data.append(TOA_BRF)\n",
    "\n",
    "    # loop over radiance bands for ravelled OLCI_data array:    \n",
    "    OLCI_data_r=[]\n",
    "    for Radiance in range(1,22):\n",
    "        Rstr=\"%02d\" % Radiance\n",
    "        OLCI_nc=Dataset(OLCI_file_p+'/Oa'+Rstr+'_radiance.nc')\n",
    "        OLCI_data_r.append(np.asarray(OLCI_nc['Oa'+Rstr+'_radiance']).ravel())\n",
    "        CNN_training[OLCI_file]['Oa'+Rstr+'_radiance']=np.asarray(OLCI_nc['Oa'+Rstr+'_radiance'])\n",
    "    \n",
    "    CNN_training[OLCI_file]['OLCI_data']=OLCI_data\n",
    "    CNN_training[OLCI_file]['OLCI_data_r']=OLCI_data_r\n",
    "    \n",
    "    print(np.shape(OLCI_data))\n",
    "    print(np.shape(OLCI_data_r))\n",
    "       \n",
    "    # need to find corresponsing SAR file (timecodes are different):\n",
    "    SAR_file=matching_SAR_list[j]\n",
    "    SAR_file_list.append(SAR_file)\n",
    "    print('Overlapping SAR file is:',SAR_file)\n",
    "    SAR_data=Dataset(dir_path+'GPOD_PROCESSED_test/201803/'+SAR_file)\n",
    "#    SAR_data=Dataset('/Volumes/cpdata/SATS/RA/S3A/L1B/GPOD_PROCESSED/201803/'+SAR_file)\n",
    "#    SAR_data=Dataset(dir_path+'/Volumes/GPOD_PROCESSED/201803/'+SAR_file)\n",
    "\n",
    "    SAR_lat, SAR_lon, waves, sig_0, RIP= unpack_gpod('latitude_20Hz'), unpack_gpod('longitude_20Hz'), unpack_gpod('SAR_Echo_Data'), unpack_gpod('Sigma0_20Hz'), unpack_gpod('Substack_RIP_Data')\n",
    "    SAR_index=np.arange(np.size(SAR_lat))\n",
    "    \n",
    "# still need to do this?  \n",
    "#     find=np.where(SAR_lat >= -180)\n",
    "#     SAR_lat=SAR_lat[find]\n",
    "#     SAR_lon=SAR_lon[find]\n",
    "#     SAR_index=SAR_index[find]\n",
    "#     waves=waves[find]\n",
    "#     sig_0=sig_0[find]\n",
    "#     RIP=RIP[find]\n",
    " \n",
    "    PP=peakiness(waves)\n",
    "    SSD=calculate_SSD(RIP)\n",
    "    \n",
    "    CNN_training[OLCI_file][\"waves\"]=waves\n",
    "    CNN_training[OLCI_file][\"sig_0\"]=sig_0\n",
    "    CNN_training[OLCI_file][\"RIP\"]=RIP\n",
    "    CNN_training[OLCI_file][\"PP\"]=PP\n",
    "    CNN_training[OLCI_file][\"SSD\"]=SSD\n",
    "    CNN_training[OLCI_file][\"SAR_index\"]=SAR_index\n",
    "    \n",
    "    SL_flagfile_p=(dir_path+'GPOD_PROCESSED_test/201803/lead_floe_flags/'+SAR_file[0:-7]+'_00_lead.nc')\n",
    "#     SL_flagfile_p=('/Volumes/GPOD_PROCESSED/201803/lead_floe_flags/'+SAR_file[0:-7]+'_00_lead.nc')\n",
    "    SL_flagfile=SAR_file[0:-7]+'_00_lead.nc'\n",
    "    flag_data=Dataset(SL_flagfile_p)\n",
    "    flag=flag_data['lead_abun_1'][:]\n",
    "    SL_flagfile_list.append(SL_flagfile)\n",
    "#     flag_index= np.arange(np.size(flag))   \n",
    "#     flag=flag[find]\n",
    "\n",
    "    CNN_training[OLCI_file][\"flag_data\"]=flag_data\n",
    "    CNN_training[OLCI_file][\"flag\"]=flag\n",
    "#     CNN_training[OLCI_file][\"flag_index\"]=flag_index\n",
    "\n",
    "    # convert lons to 360 degree format\n",
    "    SAR_lon[SAR_lon<0]=360+SAR_lon[SAR_lon<0]\n",
    "    OLCI_lon[OLCI_lon<0]=360+OLCI_lon[OLCI_lon<0]\n",
    "    OLCI_lon_r[OLCI_lon_r<0]=360+OLCI_lon_r[OLCI_lon_r<0]\n",
    "    \n",
    "    # find centre of OLCI image\n",
    "    central_lat=np.min(OLCI_lat_r)+0.5*(np.max(OLCI_lat_r)-np.min(OLCI_lat_r))\n",
    "    central_lon=np.min(OLCI_lon_r)+0.5*(np.max(OLCI_lon_r)-np.min(OLCI_lon_r))\n",
    "    print('OLCI central lat:', central_lat)\n",
    "    print('OLCI central lon:', central_lon)\n",
    "   \n",
    "    # do Lambert Azimuthal Equal Area Projection (on a equal-area projection, the area is preserved but the shape is not)\n",
    "    w=1200000\n",
    "    m = Basemap(resolution='l',projection='laea',ellps='WGS84', lat_ts=0,  lat_0= central_lat, lon_0=central_lon, width=w,height=w)\n",
    "    SAR_x,SAR_y=m(SAR_lon,SAR_lat)    \n",
    "    OLCI_x,OLCI_y=m(OLCI_lon,OLCI_lat)\n",
    "    OLCI_x_r,OLCI_y_r=m(OLCI_lon_r,OLCI_lat_r)\n",
    "\n",
    "    spec = np.where(PP > 18.0)  # Same as CryoSat-2 \n",
    "    spec = np.asarray(spec)\n",
    "    spec = spec.flatten()\n",
    "\n",
    "    diff = np.where(PP < 9.0)  # Same as CryoSat-2 \n",
    "    diff = np.asarray(diff)\n",
    "    diff = diff.flatten()\n",
    "\n",
    "    spec_SL = np.where(flag==1)[0] # from Sanggyun lead (SL) detection algorithm\n",
    "    spec_SL = np.asarray(spec_SL)\n",
    "    spec_SL = spec_SL.flatten()\n",
    "\n",
    "    diff_SL = np.where(flag==0)[0]\n",
    "    diff_SL = np.asarray(diff_SL)\n",
    "    diff_SL = diff_SL.flatten()\n",
    "    \n",
    "    \n",
    "    CNN_training[OLCI_file][\"SAR_lon\"]=SAR_lon\n",
    "    CNN_training[OLCI_file][\"SAR_lat\"]=SAR_lat\n",
    "    CNN_training[OLCI_file][\"SAR_x\"]=SAR_x\n",
    "    CNN_training[OLCI_file][\"SAR_y\"]=SAR_y\n",
    "    \n",
    "    CNN_training[OLCI_file][\"OLCI_lon\"]=OLCI_lon\n",
    "    CNN_training[OLCI_file][\"OLCI_lat\"]=OLCI_lat\n",
    "    CNN_training[OLCI_file][\"OLCI_x\"]=OLCI_x\n",
    "    CNN_training[OLCI_file][\"OLCI_y\"]=OLCI_y\n",
    "    CNN_training[OLCI_file][\"OLCI_lon_r\"]=OLCI_lon_r\n",
    "    CNN_training[OLCI_file][\"OLCI_lat_r\"]=OLCI_lat_r\n",
    "    CNN_training[OLCI_file][\"OLCI_x_r\"]=OLCI_x_r\n",
    "    CNN_training[OLCI_file][\"OLCI_y_r\"]=OLCI_y_r\n",
    "    \n",
    "    CNN_training[OLCI_file][\"spec\"]=spec\n",
    "    CNN_training[OLCI_file][\"diff\"]=diff\n",
    "    CNN_training[OLCI_file][\"spec_SL\"]=spec_SL\n",
    "    CNN_training[OLCI_file][\"diff_SL\"]=diff_SL\n",
    "    \n",
    "    \n",
    "    # do a KDtree to find all OLCI points within a 4km diameter (2km radius) of each SAR footprint.\n",
    "    points=(OLCI_x_r,OLCI_y_r)\n",
    "    points=np.transpose(points)\n",
    "    point_tree = spatial.cKDTree(points)\n",
    "\n",
    "    nearby_x=[]\n",
    "    nearby_y=[]\n",
    "    nearby_z=[]\n",
    "    match_indices_list=[]\n",
    "    distances=[]\n",
    "\n",
    "    for i in range(np.size(SAR_x)): \n",
    "        SAR_array=[]\n",
    "        coordinate=[SAR_x[i],SAR_y[i]]\n",
    "        \n",
    "        match_indices=point_tree.query_ball_point(coordinate, 2000) # search radius in metres\n",
    "\n",
    "        # check distance from each of the OLCI points\n",
    "        distance_array=[]\n",
    "        for match_index in match_indices: \n",
    "            dist=np.sqrt((OLCI_x_r[match_index]-SAR_x[i])**2+(OLCI_y_r[match_index]-SAR_y[i])**2)\n",
    "            distance_array.append(dist)\n",
    "        \n",
    "        # re-order the nearest OCLI pixels from lowest to highest distance from the SAR point\n",
    "        paired=tuple(zip(distance_array,match_indices))\n",
    "        sort_distance=sorted(paired,key=lambda x: x[0]) # key tells function how to sort it (x[0] is distance element)--> lambda functions are basically anonymous functions- iterate over the list\n",
    "        match_indices_o=[x[1] for x in sort_distance] # distance ordered match indices list\n",
    "        \n",
    "        # create an array of distances\n",
    "        distance_array_2=[]\n",
    "        for match_index_o in match_indices_o: \n",
    "            dist_2=np.sqrt((OLCI_x_r[match_index_o]-SAR_x[i])**2+(OLCI_y_r[match_index_o]-SAR_y[i])**2)\n",
    "            distance_array_2.append(dist_2)\n",
    "        distances.append(distance_array_2)\n",
    "        \n",
    "        # everything in terms of distance ordered indices (match_indices_o)\n",
    "        match_indices_list.append(match_indices_o)\n",
    "        nearby_x.append(OLCI_x_r[match_indices_o])   \n",
    "        nearby_y.append(OLCI_y_r[match_indices_o])\n",
    "        \n",
    "        for Radiance in range(1,22):\n",
    "            indices_array=[]\n",
    "            for match_index_o in match_indices_o:\n",
    "                indices_array.append(OLCI_data_r[Radiance-1][match_index_o])\n",
    "            SAR_array.append(indices_array)\n",
    "        nearby_z.append(SAR_array)        \n",
    "\n",
    "    CNN_training[OLCI_file][\"match_indices_list\"]=match_indices_list        \n",
    "    CNN_training[OLCI_file][\"nearby_x\"]=nearby_x        \n",
    "    CNN_training[OLCI_file][\"nearby_y\"]=nearby_y        \n",
    "    CNN_training[OLCI_file][\"nearby_z\"]=nearby_z\n",
    "    CNN_training[OLCI_file][\"distances\"]=distances  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fa7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding ravel \n",
    "for OLCI_file in OLCI_file_list: \n",
    "    if OLCI_file==OLCI_file_list[0]: # change this to look at other images/tracks  \n",
    "        OLCI_x=CNN_training[OLCI_file]['OLCI_x']\n",
    "        OLCI_x_r=CNN_training[OLCI_file]['OLCI_x_r']\n",
    "        print(OLCI_x[0,1])\n",
    "        print(OLCI_x_r[1])\n",
    "        # shows it ravels the data row by row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting OLCI pixel into image array format\n",
    "index0=[] # in flight direction (y values indexed) -\n",
    "index1=[] # accross flight direction (x values indexed) \n",
    "k=0\n",
    "for OLCI_file in OLCI_file_list:\n",
    "    index0_1=[]\n",
    "    index1_1=[]\n",
    "    k=k+1\n",
    "    for i in range((CNN_training[OLCI_file]['OLCI_x']).shape[0]):     # no.rows (y)\n",
    "        for j in range((CNN_training[OLCI_file]['OLCI_x']).shape[1]): # no.columns (x)\n",
    "            index0_1.append(i)  # gives y values for each point in row (will be the same for one row)\n",
    "            index1_1.append(j)  # gives x values for for each point in row  \n",
    "    index0.append(index0_1)\n",
    "    index1.append(index1_1)\n",
    "   \n",
    "\n",
    "    print('Number of OLCI pixels for image',k)\n",
    "    print(np.shape(index0_1))\n",
    "#     print(np.shape(index1_each))\n",
    "\n",
    "    # try it for a few OLCI pixels \n",
    "#     print((index0_1[4795637],index1_1[4795637]))\n",
    "#     print((index0_1[2066377],index1_1[2066377]))\n",
    "#     print((index0_1[387954],index1_1[387954]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceffadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which SAR points lie on image --> some that are just off the edge of image are counted as well  \n",
    "SAR_on_OLCI=[]\n",
    "j=-1\n",
    "for OLCI_file in OLCI_file_list:\n",
    "    j=j+1\n",
    "    SAR_on_OLCI_each=[]\n",
    "    for i in range(np.size(CNN_training[OLCI_file][\"SAR_lon\"])):       \n",
    "            if len(CNN_training[OLCI_file]['match_indices_list'][i])>0:\n",
    "                SAR_on_OLCI_each.append(i)\n",
    "    SAR_on_OLCI.append(SAR_on_OLCI_each)            \n",
    "\n",
    "    min_value=min(SAR_on_OLCI_each)\n",
    "    max_value=max(SAR_on_OLCI_each)\n",
    "    print('The SAR points that lie over the OLCI image for', j+1, 'are approximately between:', min_value, 'to', max_value, '(approx', np.shape(SAR_on_OLCI_each), 'points)')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e929dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=-1\n",
    "for OLCI_file in OLCI_file_list: \n",
    "    k=k+1\n",
    "    match_indices_list=CNN_training[OLCI_file]['match_indices_list']\n",
    "    SAR_on_OLCI_n=SAR_on_OLCI[k]   \n",
    "    min_value_n=(min(SAR_on_OLCI_n)+50)\n",
    "    max_value_n=(max(SAR_on_OLCI_n)-50)\n",
    "        \n",
    "    close_OLCI_1=match_indices_list[min_value_n][0]\n",
    "    close_OLCI_2=match_indices_list[max_value_n][0]\n",
    "        \n",
    "    # using the OLCI points closest to SAR points at edges of images \n",
    "    print('Two OLCI pixel points in array format where SAR overlaps for',k+1)\n",
    "    print((index0[k][close_OLCI_1]),(index1[k][close_OLCI_1])) \n",
    "    print((index0[k][close_OLCI_2]),(index1[k][close_OLCI_2])) \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "for OLCI_file in OLCI_file_list: \n",
    "    if OLCI_file==OLCI_file_list[0]: # change this to look at other images/tracks\n",
    "        OLCI_lat=CNN_training[OLCI_file]['OLCI_lat']\n",
    "        OLCI_lon=CNN_training[OLCI_file]['OLCI_lon']\n",
    "\n",
    "        print('Distance between consecutive OLCI points in flight direction (y direction):')\n",
    "        coords_1 = (OLCI_lat[100,100], OLCI_lon[100,100])\n",
    "        coords_2 = (OLCI_lat[99,100], OLCI_lon[99,100])\n",
    "        result= geopy.distance.distance(coords_1, coords_2).m\n",
    "        print(result)\n",
    "\n",
    "        coords_1 = (OLCI_lat[1000,1000], OLCI_lon[1000,1000])\n",
    "        coords_2 = (OLCI_lat[999,1000], OLCI_lon[999,1000])\n",
    "        result= geopy.distance.distance(coords_1, coords_2).m\n",
    "        print(result)\n",
    "\n",
    "        coords_1 = (OLCI_lat[1000,2000], OLCI_lon[1000,2000])\n",
    "        coords_2 = (OLCI_lat[999,2000], OLCI_lon[999,2000])\n",
    "        result= geopy.distance.distance(coords_1, coords_2).m\n",
    "        print(result)\n",
    "\n",
    "        coords_1 = (OLCI_lat[1100,4000], OLCI_lon[1100,4000])\n",
    "        coords_2 = (OLCI_lat[1099,4000], OLCI_lon[1099,4000])\n",
    "        result= geopy.distance.distance(coords_1, coords_2).m\n",
    "        print(result)\n",
    "        print('')\n",
    "\n",
    "        print('Distance between consecutive OLCI points across flight direction (x direction):')\n",
    "        coords_1 = (OLCI_lat[100,100], OLCI_lon[100,100])\n",
    "        coords_2 = (OLCI_lat[100,99], OLCI_lon[100,99])\n",
    "        result= geopy.distance.distance(coords_1, coords_2).m\n",
    "        print(result)\n",
    "\n",
    "        coords_1 = (OLCI_lat[1000,1000], OLCI_lon[1000,1000])\n",
    "        coords_2 = (OLCI_lat[1000,999], OLCI_lon[1000,999])\n",
    "        result= geopy.distance.distance(coords_1, coords_2).m\n",
    "        print(result)\n",
    "\n",
    "        coords_1 = (OLCI_lat[1000,2000], OLCI_lon[1000,2000])\n",
    "        coords_2 = (OLCI_lat[1000,1999], OLCI_lon[1000,1999])\n",
    "        result= geopy.distance.distance(coords_1, coords_2).m\n",
    "        print(result)\n",
    "\n",
    "        coords_1 = (OLCI_lat[1100,4000], OLCI_lon[1100,4000])\n",
    "        coords_2 = (OLCI_lat[1100,3999], OLCI_lon[1100,3999])\n",
    "        result= geopy.distance.distance(coords_1, coords_2).m\n",
    "        print(result)\n",
    "\n",
    "        coords_1 = (OLCI_lat[1100,1100], OLCI_lon[1100,1100])\n",
    "        coords_2 = (OLCI_lat[1100,1099], OLCI_lon[1100,1099])\n",
    "        result= geopy.distance.distance(coords_1, coords_2).m\n",
    "        print(result)\n",
    "\n",
    "        coords_1 = (OLCI_lat[1100,1100], OLCI_lon[1100,1100])\n",
    "        coords_2 = (OLCI_lat[1100,1101], OLCI_lon[1100,1101])\n",
    "        result= geopy.distance.distance(coords_1, coords_2).m\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff260b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for OLCI_file in OLCI_file_list: \n",
    "    if OLCI_file==OLCI_file_list[0]: # change this to look at other images/tracks\n",
    "        OLCI_y=CNN_training[OLCI_file]['OLCI_y']\n",
    "        OLCI_x=CNN_training[OLCI_file]['OLCI_x']\n",
    "           \n",
    "        print('Distance between consecutive OLCI points in flight direction (y direction):')\n",
    "        print(np.sqrt((OLCI_x[100,100]-OLCI_x[99,100])**2+((OLCI_y[100,100]-OLCI_y[99,100])**2)))\n",
    "        print(np.sqrt((OLCI_x[1000,1000]-OLCI_x[999,1000])**2+((OLCI_y[1000,1000]-OLCI_y[999,1000])**2)))\n",
    "        print(np.sqrt((OLCI_x[1000,2000]-OLCI_x[999,2000])**2+((OLCI_y[1000,2000]-OLCI_y[999,2000])**2)))\n",
    "        print(np.sqrt((OLCI_x[1100,4000]-OLCI_x[1099,4000])**2+((OLCI_y[1100,4000]-OLCI_y[1099,4000])**2)))\n",
    "        print('')\n",
    "\n",
    "        print('Distance between consecutive OLCI points across flight direction (x direction):')\n",
    "        print(np.sqrt((OLCI_x[100,100]-OLCI_x[100,99])**2+((OLCI_y[100,100]-OLCI_y[100,99])**2)))\n",
    "        print(np.sqrt((OLCI_x[1000,1000]-OLCI_x[1000,999])**2+((OLCI_y[1000,1000]-OLCI_y[1000,999])**2)))\n",
    "        print(np.sqrt((OLCI_x[1000,2000]-OLCI_x[1000,1999])**2+((OLCI_y[1000,2000]-OLCI_y[1000,1999])**2)))\n",
    "        print(np.sqrt((OLCI_x[1100,4000]-OLCI_x[1100,3999])**2+((OLCI_y[1100,4000]-OLCI_y[1100,3999])**2)))\n",
    "\n",
    "        print(np.sqrt((OLCI_x[1100,1100]-OLCI_x[1100,1099])**2+((OLCI_y[1100,1100]-OLCI_y[1100,1099])**2))) \n",
    "        print(np.sqrt((OLCI_x[1100,1100]-OLCI_x[1100,1101])**2+((OLCI_y[1100,1100]-OLCI_y[1100,1101])**2))) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# As input, a CNN takes tensors of shape (image_height, image_width, color_channels)\n",
    "# N is the number of training images, Nv is the number of validation images, Nt is the number of test images\n",
    "# train_images=np.zeros((N,11,11,21)) \n",
    "# validation_images=np.zeros((Nv,11,11,20))\n",
    "# test_images=np.zeros((Nt,11,11,20))\n",
    "# train_labels=np.zeros((N))\n",
    "# validation_labels=np.zeros((Nv))\n",
    "# train_labels=np.zeros((Nt))\n",
    "\n",
    "\n",
    "# no seperation between files \n",
    "images_n=[] # all 11x11 images (not array)\n",
    "labels_n=[] # labels for 11x11 images (not array)\n",
    "indices_n=[] # all SAR indices which correspond to 11x11 images \n",
    "files_n=[] # file number associated with each element in lists above \n",
    "\n",
    "# each element is for one file \n",
    "images_k=[]\n",
    "labels_k=[]\n",
    "indices_k=[]\n",
    "\n",
    "n=-1\n",
    "for OLCI_file in OLCI_file_list:\n",
    "    n=n+1\n",
    "    match_indices_list=CNN_training[OLCI_file]['match_indices_list'] \n",
    "    flag=CNN_training[OLCI_file]['flag']\n",
    "    OLCI_data=CNN_training[OLCI_file]['OLCI_data']    \n",
    "    SAR_lon=CNN_training[OLCI_file]['SAR_lon']\n",
    "    # for each file \n",
    "    images_e=[] \n",
    "    labels_e=[]\n",
    "    indices_e=[]     \n",
    "    for i in range(np.size(SAR_lon)):\n",
    "        if len(match_indices_list[i])>0:\n",
    "            OLCI_closest=match_indices_list[i][0] # index of closest OLCI point to SAR point\n",
    "            i0=index0[n][OLCI_closest]\n",
    "            i1=index1[n][OLCI_closest]   \n",
    "            image_temp=np.zeros((67,67,21))\n",
    "            if OLCI_data[0][i0-33:i0+34,i1-33:i1+34].shape==(67,67):\n",
    "                for r in range(1,22):            \n",
    "                    image_temp[:,:,r-1]=OLCI_data[r-1][i0-33:i0+34,i1-33:i1+34]\n",
    "                images_n.append(image_temp)   \n",
    "                labels_n.append(flag[i])\n",
    "                indices_n.append(i)\n",
    "                files_n.append(n+1) # file naming starts from 1\n",
    "                images_e.append(image_temp)   \n",
    "                labels_e.append(flag[i])\n",
    "                indices_e.append(i)                \n",
    "    images_k.append(images_e)   \n",
    "    labels_k.append(labels_e)\n",
    "    indices_k.append(indices_e)\n",
    "    \n",
    "    \n",
    "# each element is for one file- can use for checks\n",
    "labels_k1=[]\n",
    "images_k1=[]\n",
    "indices_k1=[]\n",
    "labels_k0=[]\n",
    "images_k0=[]\n",
    "indices_k0=[]   \n",
    "for i in range(len(indices_k)):    \n",
    "    labels_k_e=np.asarray(labels_k[i])\n",
    "    images_k_e=np.asarray(images_k[i])\n",
    "    indices_k_e=np.asarray(indices_k[i])\n",
    "    \n",
    "    labels_k1_e=labels_k_e[labels_k_e==1]\n",
    "    images_k1_e=images_k_e[labels_k_e==1]\n",
    "    indices_k1_e=indices_k_e[labels_k_e==1]\n",
    "    labels_k0_e=labels_k_e[labels_k_e==0]\n",
    "    images_k0_e=images_k_e[labels_k_e==0]\n",
    "    indices_k0_e=indices_k_e[labels_k_e==0]\n",
    "    \n",
    "    labels_k1.append(labels_k1_e)\n",
    "    images_k1.append(images_k1_e)\n",
    "    indices_k1.append(indices_k1_e)\n",
    "    labels_k0.append(labels_k0_e)\n",
    "    images_k0.append(images_k0_e)\n",
    "    indices_k0.append(indices_k0_e)\n",
    "\n",
    "\n",
    "# turning lists into arrays \n",
    "images_n_a=np.asarray(images_n)\n",
    "indices_n_a=np.asarray(indices_n)\n",
    "labels_n_a=np.asarray(labels_n)\n",
    "files_n_a=np.asarray(files_n)    \n",
    "\n",
    "# using equal number of observations for each class\n",
    "labels1=labels_n_a[labels_n_a==1.0]\n",
    "labels0_a=labels_n_a[labels_n_a==0.0]\n",
    "n=len(labels1)\n",
    "labels0=labels0_a[:n]\n",
    "\n",
    "images1=images_n_a[labels_n_a==1.0,:,:,:]\n",
    "images0_a=images_n_a[labels_n_a==0.0,:,:,:]\n",
    "images0=images0_a[:n]\n",
    "\n",
    "indices1=indices_n_a[labels_n_a==1.0]\n",
    "indices0_a=indices_n_a[labels_n_a==0.0]\n",
    "indices0=indices0_a[:n]\n",
    "\n",
    "files1=files_n_a[labels_n_a==1.0]\n",
    "files0_a=files_n_a[labels_n_a==0.0]\n",
    "files0=files0_a[:n]\n",
    "\n",
    "# creating 50/50 lead/sea-ice datasets    \n",
    "labels_cnn=np.concatenate((labels1,labels0))\n",
    "files_cnn=np.concatenate((files1,files0))\n",
    "indices_cnn=np.concatenate((indices1,indices0))\n",
    "images_cnn=np.concatenate((images1,images0))\n",
    "\n",
    "print(images_cnn.shape)\n",
    "    \n",
    "# ###SPLITTING INTO TRAINING, VALIDATION AND TESTING DATA###\n",
    "# random_state simply sets a seed to the random generator, so that your train-test splits are always deterministic. If you don't set a seed, it is different each time\n",
    "train_images, test_images, train_labels, test_labels, train_indices, test_indices, train_files, test_files = train_test_split(images_cnn, labels_cnn, indices_cnn, files_cnn, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eea34dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data in the form of numpy arrays\n",
    "np.save('x_train96', train_images)\n",
    "np.save('x_test96', test_images)\n",
    "np.save('y_train96', train_labels)\n",
    "np.save('y_test96', test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
